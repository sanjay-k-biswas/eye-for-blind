{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjay-k-biswas/eye-for-blind/blob/main/Eye_for_blind_workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-LPUX0UgbYr"
      },
      "source": [
        "# EYE FOR BLIND\n",
        "This notebook will be used to prepare the capstone project 'Eye for Blind'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The World Health Organization (WHO) has reported that approximately 285 million people are visually impaired worldwide, and out of these 285 million, 39 million are completely blind. It gets extremely tough for them to carry out daily activities, one of which is reading. From reading a newspaper or a magazine to reading an important text message from your bank, it is tough for them to read the text written in it.\n",
        "\n",
        "A similar problem they also face is seeing and enjoying the beauty of pictures and images. Today, in the world of social media, millions of images are uploaded daily. Some of them are about your friends and family, while some of them are about nature and its beauty. Understanding what is present in that image is quite a challenge for certain people who are suffering from visual impairment or who are blind.\n",
        "\n",
        "---\n",
        "\n",
        "**Problem Statement**\n",
        "\n",
        "In this capstone project, you need to create a deep learning model which can explain the contents of an image in the form of speech through caption generation with an attention mechanism on Flickr8K dataset. This kind of model is a use-case for blind people so that they can understand any image with the help of speech. The caption generated through a CNN-RNN model will be converted to speech using a text to speech library. \n",
        "\n",
        "This problem statement is an application of both deep learning and natural language processing. The features of an image will be extracted by a CNN-based encoder and this will be decoded by an RNN model.\n",
        "\n",
        "The project is an extended application of [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044) paper.\n",
        "\n",
        " \n",
        "\n",
        "The dataset is taken from the [Kaggle website](https://www.kaggle.com/adityajn105/flickr8k) and it consists of sentence-based image descriptions having a list of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events of the image."
      ],
      "metadata": {
        "id": "rxeoMV2Xgrp7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxOYTzxlgbYu"
      },
      "outputs": [],
      "source": [
        "#Import all the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import keras\n",
        "import glob\n",
        "import os\n",
        "import random as rn \n",
        "rn.seed(42)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from collections import Counter\n",
        "import string\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ4J07VtgbYw"
      },
      "source": [
        "## Data understanding\n",
        "1.Import the dataset and read image & captions into two seperate variables\n",
        "\n",
        "2.Visualise both the images & text present in the dataset\n",
        "\n",
        "3.Create a dataframe which summarizes the image, path & captions as a dataframe\n",
        "\n",
        "4.Create a list which contains all the captions & path\n",
        "\n",
        "5.Visualise the top 30 occuring words in the captions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "dBZCufDV8utj",
        "outputId": "36409026-e112-42fd-8fbf-753b84fd1a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8fd36b41-3e71-4aa4-acdf-c51e2bd7bd12\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8fd36b41-3e71-4aa4-acdf-c51e2bd7bd12\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"sanjaybiswas\",\"key\":\"a68c3fff4c1444131ecc160aebdd3459\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets list -s 'Flickr 8k'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsKCFzJY9M5v",
        "outputId": "48dbbc00-a2d4-4475-e602-47e443ecec5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "ref                                                         title                                            size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------  ----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "adityajn105/flickr8k                                        Flickr 8k Dataset                                 1GB  2020-04-27 07:27:19          26854        197  0.75             \n",
            "kunalgupta2616/flickr-8k-images-with-captions               Flickr 8K Images with Captions                    1GB  2020-09-12 13:44:07           1612         10  0.9375           \n",
            "gazu468/flickr-8k-images                                    Flickr 8k Dataset                                 1GB  2021-10-23 12:54:09             18          9  0.5625           \n",
            "dibyansudiptiman/flickr-8k                                  Flickr8k Image Dataset                           53MB  2020-08-06 15:05:46            353          5  1.0              \n",
            "tanreinama/flicker8k-japanese-translation                   Flicker8K Japanese Translation                   39MB  2022-10-15 07:04:53             10          7  0.8235294        \n",
            "abhishekravi/flickr-8k                                      Flickr-8k-zip                                     1GB  2021-03-18 05:04:14             34          1  0.1875           \n",
            "abhijeetrojatkar/imagecap01                                 image-cap01                                     131MB  2020-07-19 07:49:33              4          1  0.375            \n",
            "gondimjoaom/flickr-8k-dataset-traduzido-pt-automaticamente  Flickr 8K Dataset traduzido PT automaticamente    2GB  2021-09-11 18:57:43              3          0  0.5              \n",
            "dhamodharreddy/image-captioning                             Image captioning                                  1GB  2022-05-07 07:02:08              0          1  0.1875           \n",
            "sakshighadigaonkar/flickr-8k                                Flickr_8k                                         1GB  2021-03-25 03:46:24             87          2  0.1875           \n",
            "lakshmisruthikotha/flickr-8k-dataset                        Flickr_8k_Dataset                                57KB  2020-09-21 09:43:27              7          0  0.125            \n",
            "rohit101294/flickr-8k-dataset                               Flickr_8k_dataset                                 1GB  2021-11-11 17:17:47              1          0  0.125            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d adityajn105/flickr8k -p /content/kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT7F5PXt9unu",
        "outputId": "27399231-8493-4ff6-e97b-848ebb7dcba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flickr8k.zip to /content/kaggle\n",
            " 99% 1.03G/1.04G [00:05<00:00, 233MB/s]\n",
            "100% 1.04G/1.04G [00:05<00:00, 201MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/flickr8k.zip"
      ],
      "metadata": {
        "id": "mqeketZY-llO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/kaggle/flickr8k.zip"
      ],
      "metadata": {
        "id": "q9JolSdI_SvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSMKetx2gbYw"
      },
      "source": [
        "Let's read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvdoeojvgbYw",
        "outputId": "d435f3bf-f9ae-4294-f3f2-97d8ab0969ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total images present in the dataset: 0\n"
          ]
        }
      ],
      "source": [
        "#Import the dataset and read the image into a separate variable\n",
        "images='content/Images/'\n",
        "\n",
        "all_imgs = glob.glob(images + '/*.jpg',recursive=True)\n",
        "print(\"The total images present in the dataset: {}\".format(len(all_imgs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IJz74X7gbYx"
      },
      "outputs": [],
      "source": [
        "#Visualise both the images & text present in the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W6urGe5gbYx",
        "outputId": "493bbbae-68a5-4d50-db2a-7f91abda940e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        image  \\\n",
            "0   1000268201_693b08cb0e.jpg   \n",
            "1   1000268201_693b08cb0e.jpg   \n",
            "2   1000268201_693b08cb0e.jpg   \n",
            "3   1000268201_693b08cb0e.jpg   \n",
            "4   1000268201_693b08cb0e.jpg   \n",
            "5   1001773457_577c3a7d70.jpg   \n",
            "6   1001773457_577c3a7d70.jpg   \n",
            "7   1001773457_577c3a7d70.jpg   \n",
            "8   1001773457_577c3a7d70.jpg   \n",
            "9   1001773457_577c3a7d70.jpg   \n",
            "10  1002674143_1b742ab4b8.jpg   \n",
            "11  1002674143_1b742ab4b8.jpg   \n",
            "12  1002674143_1b742ab4b8.jpg   \n",
            "13  1002674143_1b742ab4b8.jpg   \n",
            "14  1002674143_1b742ab4b8.jpg   \n",
            "15  1003163366_44323f5815.jpg   \n",
            "16  1003163366_44323f5815.jpg   \n",
            "17  1003163366_44323f5815.jpg   \n",
            "18  1003163366_44323f5815.jpg   \n",
            "19  1003163366_44323f5815.jpg   \n",
            "20  1007129816_e794419615.jpg   \n",
            "21  1007129816_e794419615.jpg   \n",
            "22  1007129816_e794419615.jpg   \n",
            "23  1007129816_e794419615.jpg   \n",
            "24  1007129816_e794419615.jpg   \n",
            "25  1007320043_627395c3d8.jpg   \n",
            "26  1007320043_627395c3d8.jpg   \n",
            "27  1007320043_627395c3d8.jpg   \n",
            "28  1007320043_627395c3d8.jpg   \n",
            "29  1007320043_627395c3d8.jpg   \n",
            "\n",
            "                                              caption  \n",
            "0   A child in a pink dress is climbing up a set o...  \n",
            "1               A girl going into a wooden building .  \n",
            "2    A little girl climbing into a wooden playhouse .  \n",
            "3   A little girl climbing the stairs to her playh...  \n",
            "4   A little girl in a pink dress going into a woo...  \n",
            "5          A black dog and a spotted dog are fighting  \n",
            "6   A black dog and a tri-colored dog playing with...  \n",
            "7   A black dog and a white dog with brown spots a...  \n",
            "8   Two dogs of different breeds looking at each o...  \n",
            "9     Two dogs on pavement moving toward each other .  \n",
            "10  A little girl covered in paint sits in front o...  \n",
            "11  A little girl is sitting in front of a large p...  \n",
            "12  A small girl in the grass plays with fingerpai...  \n",
            "13  There is a girl with pigtails sitting in front...  \n",
            "14  Young girl with pigtails painting outside in t...  \n",
            "15  A man lays on a bench while his dog sits by him .  \n",
            "16  A man lays on the bench to which a white dog i...  \n",
            "17  a man sleeping on a bench outside with a white...  \n",
            "18  A shirtless man lies on a park bench with his ...  \n",
            "19  man laying on bench holding leash of dog sitti...  \n",
            "20     A man in an orange hat starring at something .  \n",
            "21            A man wears an orange hat and glasses .  \n",
            "22  A man with gauges and glasses is wearing a Bli...  \n",
            "23  A man with glasses is wearing a beer can croch...  \n",
            "24  The man with pierced ears is wearing glasses a...  \n",
            "25                    A child playing on a rope net .  \n",
            "26             A little girl climbing on red roping .  \n",
            "27  A little girl in pink climbs a rope bridge at ...  \n",
            "28  A small child grips onto the red ropes at the ...  \n",
            "29  The small child climbs on a red ropes on a pla...  \n",
            "                        image  \\\n",
            "0   1000268201_693b08cb0e.jpg   \n",
            "1   1000268201_693b08cb0e.jpg   \n",
            "2   1000268201_693b08cb0e.jpg   \n",
            "3   1000268201_693b08cb0e.jpg   \n",
            "4   1000268201_693b08cb0e.jpg   \n",
            "5   1001773457_577c3a7d70.jpg   \n",
            "6   1001773457_577c3a7d70.jpg   \n",
            "7   1001773457_577c3a7d70.jpg   \n",
            "8   1001773457_577c3a7d70.jpg   \n",
            "9   1001773457_577c3a7d70.jpg   \n",
            "10  1002674143_1b742ab4b8.jpg   \n",
            "11  1002674143_1b742ab4b8.jpg   \n",
            "12  1002674143_1b742ab4b8.jpg   \n",
            "13  1002674143_1b742ab4b8.jpg   \n",
            "14  1002674143_1b742ab4b8.jpg   \n",
            "15  1003163366_44323f5815.jpg   \n",
            "16  1003163366_44323f5815.jpg   \n",
            "17  1003163366_44323f5815.jpg   \n",
            "18  1003163366_44323f5815.jpg   \n",
            "19  1003163366_44323f5815.jpg   \n",
            "20  1007129816_e794419615.jpg   \n",
            "21  1007129816_e794419615.jpg   \n",
            "22  1007129816_e794419615.jpg   \n",
            "23  1007129816_e794419615.jpg   \n",
            "24  1007129816_e794419615.jpg   \n",
            "25  1007320043_627395c3d8.jpg   \n",
            "26  1007320043_627395c3d8.jpg   \n",
            "27  1007320043_627395c3d8.jpg   \n",
            "28  1007320043_627395c3d8.jpg   \n",
            "29  1007320043_627395c3d8.jpg   \n",
            "\n",
            "                                              caption  \n",
            "0   A child in a pink dress is climbing up a set o...  \n",
            "1               A girl going into a wooden building .  \n",
            "2    A little girl climbing into a wooden playhouse .  \n",
            "3   A little girl climbing the stairs to her playh...  \n",
            "4   A little girl in a pink dress going into a woo...  \n",
            "5          A black dog and a spotted dog are fighting  \n",
            "6   A black dog and a tri-colored dog playing with...  \n",
            "7   A black dog and a white dog with brown spots a...  \n",
            "8   Two dogs of different breeds looking at each o...  \n",
            "9     Two dogs on pavement moving toward each other .  \n",
            "10  A little girl covered in paint sits in front o...  \n",
            "11  A little girl is sitting in front of a large p...  \n",
            "12  A small girl in the grass plays with fingerpai...  \n",
            "13  There is a girl with pigtails sitting in front...  \n",
            "14  Young girl with pigtails painting outside in t...  \n",
            "15  A man lays on a bench while his dog sits by him .  \n",
            "16  A man lays on the bench to which a white dog i...  \n",
            "17  a man sleeping on a bench outside with a white...  \n",
            "18  A shirtless man lies on a park bench with his ...  \n",
            "19  man laying on bench holding leash of dog sitti...  \n",
            "20     A man in an orange hat starring at something .  \n",
            "21            A man wears an orange hat and glasses .  \n",
            "22  A man with gauges and glasses is wearing a Bli...  \n",
            "23  A man with glasses is wearing a beer can croch...  \n",
            "24  The man with pierced ears is wearing glasses a...  \n",
            "25                    A child playing on a rope net .  \n",
            "26             A little girl climbing on red roping .  \n",
            "27  A little girl in pink climbs a rope bridge at ...  \n",
            "28  A small child grips onto the red ropes at the ...  \n",
            "29  The small child climbs on a red ropes on a pla...  \n"
          ]
        }
      ],
      "source": [
        "#Import the dataset and read the text file into a seperate variable\n",
        "\n",
        "def load_doc(filename):\n",
        "    text_file_path = 'gdrive/My Drive/AI/eye-for-blind/archive/'\n",
        "\n",
        "    text = pd.read_csv(text_file_path + filename)\n",
        "    \n",
        "    return text\n",
        "\n",
        "doc = load_doc('captions.txt')\n",
        "print(doc[:30])\n",
        "print(doc.head(30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuXEjFXAgbYy"
      },
      "source": [
        "Create a dataframe which summarizes the image, path & captions as a dataframe\n",
        "\n",
        "Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjhSiSjcgbYy"
      },
      "outputs": [],
      "source": [
        "all_img_id= #store all the image id here\n",
        "all_img_vector= #store all the image path here\n",
        "annotations= #store all the captions here\n",
        "\n",
        "df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n",
        "    \n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud4l4bdCgbYy"
      },
      "outputs": [],
      "source": [
        "#Create a list which contains all the captions\n",
        "annotations=#write your code here\n",
        "\n",
        "#add the <start> & <end> token to all those captions as well\n",
        "\n",
        "#Create a list which contains all the path to the images\n",
        "all_img_path=#write your code here\n",
        "\n",
        "print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n",
        "print(\"Total images present in the dataset: \" + str(len(all_img_path)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWTwKltzgbYz"
      },
      "outputs": [],
      "source": [
        "#Create the vocabulary & the counter for the captions\n",
        "\n",
        "vocabulary= #write your code here\n",
        "\n",
        "val_count=Counter(vocabulary)\n",
        "val_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0KOGh21gbYz"
      },
      "outputs": [],
      "source": [
        "#Visualise the top 30 occuring words in the captions\n",
        "\n",
        "\n",
        "#write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z4LOefCgbYz"
      },
      "source": [
        "## Pre-Processing the captions\n",
        "1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \n",
        "This gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n",
        "\n",
        "2.Replace all other words with the unknown token \"UNK\" .\n",
        "\n",
        "3.Create word-to-index and index-to-word mappings.\n",
        "\n",
        "4.Pad all sequences to be the same length as the longest one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGYBY2SMgbY0"
      },
      "outputs": [],
      "source": [
        "# create the tokenizer\n",
        "\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t0xVh6EgbY0"
      },
      "outputs": [],
      "source": [
        "# Create word-to-index and index-to-word mappings.\n",
        "\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kFtie3wgbY0"
      },
      "outputs": [],
      "source": [
        "# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n",
        "\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJML4K1CgbY0"
      },
      "outputs": [],
      "source": [
        "# Pad each vector to the max_length of the captions ^ store it to a vairable\n",
        "\n",
        "cap_vector= #your code here\n",
        "\n",
        "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9viT4HfHgbY1"
      },
      "source": [
        "## Pre-processing the images\n",
        "\n",
        "1.Resize them into the shape of (299, 299)\n",
        "\n",
        "3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8LZtRFQgbY1"
      },
      "source": [
        "### FAQs on how to resize the images::\n",
        "* Since you have a list which contains all the image path, you need to first convert them to a dataset using <i>tf.data.Dataset.from_tensor_slices</i>. Once you have created a dataset consisting of image paths, you need to apply a function to the dataset which will apply the necessary preprocessing to each image. \n",
        "* This function should resize them and also should do the necessary preprocessing that it is in correct format for InceptionV3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX04iNzygbY1"
      },
      "outputs": [],
      "source": [
        "#write your code here to create the dataset consisting of image paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knmWRJX9gbY1"
      },
      "outputs": [],
      "source": [
        "#write your code here for creating the function. This function should return images & their path\n",
        "\n",
        "def load_image(image_path):\n",
        "    #write your pre-processing steps here\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6dgOzhIgbY2"
      },
      "outputs": [],
      "source": [
        "#write your code here for applying the function to the image path dataset, such that the transformed dataset should contain images & their path\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFqSkCTTgbY2"
      },
      "source": [
        "## Load the pretrained Imagenet weights of Inception net V3\n",
        "\n",
        "1.To save the memory(RAM) from getting exhausted, extract the features of the images using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n",
        "\n",
        "2.The shape of the output of this layer is 8x8x2048. \n",
        "\n",
        "3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJNI_tNFgbY3"
      },
      "outputs": [],
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
        "\n",
        "new_input = #write code here to get the input of the image_model\n",
        "hidden_layer = #write code here to get the output of the image_model\n",
        "\n",
        "image_features_extract_model = #build the final model using both input & output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mxhDFYIgbY3"
      },
      "outputs": [],
      "source": [
        "# write the code to apply the feature_extraction model to your earlier created dataset which contained images & their respective paths\n",
        "# Once the features are created, you need to reshape them such that feature shape is in order of (batch_size, 8*8, 2048)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbnL4R-6gbY3"
      },
      "source": [
        "### FAQs on how to store the features:\n",
        "* You can store the features using a dictionary with the path as the key and values as the feature extracted by the inception net v3 model OR\n",
        "* You can store using numpy(np.save) to store the resulting vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjtYJTscgbY4"
      },
      "source": [
        "## Dataset creation\n",
        "1.Apply train_test_split on both image path & captions to create the train & test list. Create the train-test spliit using 80-20 ratio & random state = 42\n",
        "\n",
        "2.Create a function which maps the image path to their feature. \n",
        "\n",
        "3.Create a builder function to create train & test dataset & apply the function created earlier to transform the dataset\n",
        "\n",
        "2.Make sure you have done Shuffle and batch while building the dataset\n",
        "\n",
        "3.The shape of each image in the dataset after building should be (batch_size, 8*8, 2048)\n",
        "\n",
        "4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsyrNkK3gbY4"
      },
      "outputs": [],
      "source": [
        "#write your code here\n",
        "\n",
        "path_train, path_test, cap_train, cap_test = train_test_split( #your code goes here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjUBO3UHgbY4"
      },
      "outputs": [],
      "source": [
        "print(\"Training data for images: \" + str(len(path_train)))\n",
        "print(\"Testing data for images: \" + str(len(path_test)))\n",
        "print(\"Training data for Captions: \" + str(len(cap_train)))\n",
        "print(\"Testing data for Captions: \" + str(len(cap_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmCm02sDgbY5"
      },
      "outputs": [],
      "source": [
        "# Create a function which maps the image path to their feature. \n",
        "# This function will take the image_path & caption and return it's feature & respective caption.\n",
        "\n",
        "def map_func(# your input variable goes here):\n",
        "  img_tensor = # write your code here to extract the features from the dictionary stored earlier\n",
        "  return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxmnM7ggbY5"
      },
      "source": [
        "### FAQs on how to load the features:\n",
        "* You can load the features using the dictionary created earlier OR\n",
        "* You can store using numpy(np.load) to load the feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6phHRScygbY5"
      },
      "outputs": [],
      "source": [
        "# create a builder function to create dataset which takes in the image path & captions as input\n",
        "# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n",
        "\n",
        "def gen_dataset(#your input variables):\n",
        "    \n",
        "    # your code goes here to create the dataset & transform it\n",
        "    \n",
        "    \n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzZVSr0CgbY5"
      },
      "outputs": [],
      "source": [
        "train_dataset=gen_dataset(path_train,cap_train)\n",
        "test_dataset=gen_dataset(path_test,cap_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4sUyjqwgbY6"
      },
      "outputs": [],
      "source": [
        "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
        "print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\n",
        "print(sample_cap_batch.shape) #(batch_size,max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQxc-4K5gbY6"
      },
      "source": [
        "## Model Building\n",
        "1.Set the parameters\n",
        "\n",
        "2.Build the Encoder, Attention model & Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7i5QIjfgbY6"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256 \n",
        "units = 512\n",
        "vocab_size = #top 5,000 words +1\n",
        "train_num_steps = =#len(total train images) // BATCH_SIZE\n",
        "test_num_steps = #len(total test images) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExHGQYQygbY6"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otYjJdRbgbY6"
      },
      "outputs": [],
      "source": [
        "class Encoder(Model):\n",
        "    def __init__(self,embed_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dense = #build your Dense layer with relu activation\n",
        "        \n",
        "    def call(self, features):\n",
        "        features =  # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
        "        \n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01jjV886gbY6"
      },
      "outputs": [],
      "source": [
        "encoder=Encoder(embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0D3g_uCgbY7"
      },
      "source": [
        "### Attention model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLyvgow7gbY7"
      },
      "outputs": [],
      "source": [
        "class Attention_model(Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention_model, self).__init__()\n",
        "        self.W1 = #build your Dense layer\n",
        "        self.W2 = #build your Dense layer\n",
        "        self.V = #build your final Dense layer with unit 1\n",
        "        self.units=units\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        #features shape: (batch_size, 8*8, embedding_dim)\n",
        "        # hidden shape: (batch_size, hidden_size)\n",
        "        hidden_with_time_axis =  # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
        "        score = # build your score funciton to shape: (batch_size, 8*8, units)\n",
        "        attention_weights =  # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
        "        context_vector =  #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
        "        context_vector = # reduce the shape to (batch_size, embedding_dim)\n",
        "        \n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdLGbKOcgbY7"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK6_59qXgbY7"
      },
      "outputs": [],
      "source": [
        "class Decoder(Model):\n",
        "    def __init__(self, embed_dim, units, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.units=units\n",
        "        self.attention = #iniitalise your Attention model with units\n",
        "        self.embed = #build your Embedding layer\n",
        "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "        self.d1 = #build your Dense layer\n",
        "        self.d2 = #build your Dense layer\n",
        "        \n",
        "\n",
        "    def call(self,x,features, hidden):\n",
        "        context_vector, attention_weights = #create your context vector & attention weights from attention model\n",
        "        embed =  # embed your input to shape: (batch_size, 1, embedding_dim)\n",
        "        embed =  # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
        "        output,state = # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n",
        "        output = self.d1(output)\n",
        "        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n",
        "        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n",
        "        \n",
        "        return output,state, attention_weights\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ym8AkzvgbY7"
      },
      "outputs": [],
      "source": [
        "decoder=Decoder(embedding_dim, units, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3Ub2F0IgbY7"
      },
      "outputs": [],
      "source": [
        "features=encoder(sample_img_batch)\n",
        "\n",
        "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
        "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
        "\n",
        "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
        "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
        "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
        "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj661WTNgbY8"
      },
      "source": [
        "## Model training & optimization\n",
        "1.Set the optimizer & loss object\n",
        "\n",
        "2.Create your checkpoint path\n",
        "\n",
        "3.Create your training & testing step functions\n",
        "\n",
        "4.Create your loss function for the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJGJV7J2gbY8"
      },
      "outputs": [],
      "source": [
        "optimizer = #define the optimizer\n",
        "loss_object = #define your loss object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDNSMR_ugbY8"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dveR9hVgbY8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"your checkpoint path\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path_ckpt, max_to_keep=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqSyeX1AgbY8"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFcc7hJVgbY9"
      },
      "source": [
        "* While creating the training step for your model, you will apply Teacher forcing.\n",
        "* Teacher forcing is a technique where the target/real word is passed as the next input to the decoder instead of previous prediciton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmHozt8XgbY9"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        #write your code here to do the training steps\n",
        "        \n",
        "    return loss, avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB3zLUv2gbY9"
      },
      "source": [
        "* While creating the test step for your model, you will pass your previous prediciton as the next input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfjJf8yygbY9"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    \n",
        "    #write your code here to do the testing steps\n",
        "        \n",
        "    return loss, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k8zioBXgbY9"
      },
      "outputs": [],
      "source": [
        "def test_loss_cal(test_dataset):\n",
        "    total_loss = 0\n",
        "\n",
        "    #write your code to get the average loss result on your test data\n",
        "    \n",
        "    return avg_test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_QBwUP0gbY9"
      },
      "outputs": [],
      "source": [
        "loss_plot = []\n",
        "test_loss_plot = []\n",
        "EPOCHS = 15\n",
        "\n",
        "best_test_loss=100\n",
        "for epoch in tqdm(range(0, EPOCHS)):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "        avg_train_loss=total_loss / train_num_steps\n",
        "        \n",
        "    loss_plot.append(avg_train_loss)    \n",
        "    test_loss = test_loss_cal(test_dataset)\n",
        "    test_loss_plot.append(test_loss)\n",
        "    \n",
        "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    \n",
        "    if test_loss < best_test_loss:\n",
        "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
        "        best_test_loss = test_loss\n",
        "        ckpt_manager.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stua1vKCgbY-"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.plot(test_loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No_Tu2x-gbY-"
      },
      "source": [
        "#### NOTE: \n",
        "* Since there is a difference between the train & test steps ( Presence of teacher forcing), you may observe that the train loss is decreasing while your test loss is not. \n",
        "* This doesn't mean that the model is overfitting, as we can't compare the train & test results here, as both approach is different.\n",
        "* Also, if you want to achieve better results you can run it more epochs, but the intent of this capstone is to give you an idea on how to integrate attention mechanism with E-D architecture for images. The intent is not to create the state of art model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_hO8FGegbY-"
      },
      "source": [
        "## Model Evaluation\n",
        "1.Define your evaluation function using greedy search\n",
        "\n",
        "2.Define your evaluation function using beam search ( optional)\n",
        "\n",
        "3.Test it on a sample data using BLEU score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye5m1-jIgbY-"
      },
      "source": [
        "### Greedy Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DJQ4vkrgbY-"
      },
      "outputs": [],
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0) #process the input image to desired format before extracting features\n",
        "    img_tensor_val = # Extract features using our feature extraction model\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = # extract the features by passing the input to encoder\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = # get the output from decoder\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = #extract the predicted id(embedded value) which carries the max value\n",
        "        #map the id to the word from tokenizer and append the value to the result list\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot,predictions\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot,predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "algfeHuUgbY-"
      },
      "source": [
        "### Beam Search(optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1Y5EouAgbY_"
      },
      "outputs": [],
      "source": [
        "def beam_evaluate(image, beam_index = #your value for beam index):\n",
        "\n",
        "    #write your code to evaluate the result using beam search\n",
        "                  \n",
        "    return final_caption\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vLzWJ11gbY_"
      },
      "outputs": [],
      "source": [
        "def plot_attmap(caption, weights, image):\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    temp_img = np.array(Image.open(image))\n",
        "    \n",
        "    len_cap = len(caption)\n",
        "    for cap in range(len_cap):\n",
        "        weights_img = np.reshape(weights[cap], (8,8))\n",
        "        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n",
        "        \n",
        "        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1)\n",
        "        ax.set_title(caption[cap], fontsize=15)\n",
        "        \n",
        "        img=ax.imshow(temp_img)\n",
        "        \n",
        "        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n",
        "        ax.axis('off')\n",
        "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaAlNsADgbY_"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYVjgC15gbY_"
      },
      "outputs": [],
      "source": [
        "def filt_text(text):\n",
        "    filt=['<start>','<unk>','<end>'] \n",
        "    temp= text.split()\n",
        "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
        "    text=' '.join(temp)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9IM9z8ZgbY_"
      },
      "outputs": [],
      "source": [
        "rid = np.random.randint(0, len(img_test))\n",
        "test_image = img_test[rid]\n",
        "#test_image = './images/413231421_43833a11f5.jpg'\n",
        "#real_caption = '<start> black dog is digging in the snow <end>'\n",
        "\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test[rid] if i not in [0]])\n",
        "result, attention_plot,pred_test = evaluate(test_image)\n",
        "\n",
        "\n",
        "real_caption=filt_text(real_caption)      \n",
        "\n",
        "\n",
        "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
        "\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = pred_caption.split()\n",
        "\n",
        "score = sentence_bleu(reference, candidate, weights=#set your weights)\n",
        "print(f\"BELU score: {score*100}\")\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Prediction Caption:', pred_caption)\n",
        "plot_attmap(result, attention_plot, test_image)\n",
        "\n",
        "\n",
        "Image.open(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwKkE7UFgbY_"
      },
      "outputs": [],
      "source": [
        "captions=beam_evaluate(test_image)\n",
        "print(captions)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}